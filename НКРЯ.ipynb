{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import xml.etree.cElementTree as ET\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "import pymorphy2\n",
    "import bs4\n",
    "import collections\n",
    "import re\n",
    "import requests\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "extended_punctuation = string.punctuation + '—»«...--\"''\" '\n",
    "from stop_words import get_stop_words\n",
    "stoplist = get_stop_words('ru') + ['сей', 'свой', 'едва', 'самый', 'го', 'час', 'часы', 'ть', 'минута', 'метр'] + ['com', 'id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция, которая убирает из текста буквы английского и фрнцузского алфавита, стопслова и пунктуацию и лемматизирует\n",
    "def preprocess_text(some_text):\n",
    "    some_text = re.sub(r'\\[|\\]|([0-9])|[a-zA-ZàâäôéèëêïîçùûüÿæœÀÂÄÔÉÈËÊÏÎŸÇÙÛÜÆŒ]', '', str(some_text))\n",
    "    lemmatized_text = [morph.parse(word)[0].normal_form for word in word_tokenize(some_text.lower()) if\n",
    "                       morph.parse(word)[0].normal_form not in stoplist and extended_punctuation]\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_corpus_lemmatized = {}\n",
    "for diaries, year in zip(diaries_for_years_text.values(), range(1928, 1954)):\n",
    "    years_diaries_dic[year] = [preprocess_text(diarie) for diarie in diaries]\n",
    "    print(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#создаем словарь с публицистикой, где ключа - год, значение - список с содержанием статей\n",
    "papers_for_years = collections.defaultdict(list)\n",
    "for file_root, dirs, files in os.walk(r'C:\\Users\\kryuk\\Desktop\\учеба\\asya'):\n",
    "    for file in files:\n",
    "        if file.endswith('.xhtml') or file.endswith('.xml'):\n",
    "            print(file)\n",
    "            tree = ET.parse(os.path.join(file_root, file))\n",
    "            root = tree.getroot()\n",
    "            for tag in root.iter('meta'): \n",
    "                if tag.attrib['name'] == 'created' or tag.attrib['name'] == 'date': \n",
    "                    text = '\\n'.join([child.text for child in list(root[1]) if child.text])\n",
    "                    papers_for_years[int(re.search(r'\\d{4}', tag.attrib['content'])[0])].append(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for year in range(1921, 1925):\n",
    "#     if year in papers_for_years:\n",
    "#         print(year, '\\n', papers_for_years[year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_for_years1 = {year: [preprocess_text(note) for note in paper] for year, paper in paper_corpus_without_punct.items()\n",
    "                     if year in range(1928, 1954)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#записываем наш газетныф корпус в пикл\n",
    "import pickle\n",
    "with open('paper_corpus_without_punct.pickle', 'wb') as fw:\n",
    "    pickle.dump(papers_for_years1, fw, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('paper_corpus_lemmatized.pickle', 'rb') as f:\n",
    "    paper_corpus_lemmatized = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('paper_corpus_without_punct.pickle', 'rb') as f:\n",
    "    paper_corpus_without_punct = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_corpus_without_punct_1928 = [[word for word in paper if word not in extended_punctuation] for paper in paper_corpus_without_punct[1928]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "papers_for_years_without_punkt = {year: [[word for word in paper if word not in extended_punctuation] for paper \n",
    "                                 in papers] for year, papers in paper_corpus_without_punct.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#уплощаем структуру словая, чтобы по значениям лежал весь мешок слов всех записей в списке\n",
    "flat_dict_papers = {year: [note for paper in papers for note in paper] for year, papers in \n",
    "                    papers_for_years_without_punkt.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#то же делаем с корпусом дневников\n",
    "flat_prozhito = {year: ' '.join([' '.join(diary) for diary in diaries]) for year, diaries in years_diaries_clean_dic.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_prozhito_without_punkt = {year: ''.join([letter for letter in string if letter not in extended_punctuation]) \n",
    "                               for year, string in years_diaries_clean_dic.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(model, feature_names, no_top_words):\n",
    "    '''\n",
    "    берет модель топик-моделинга, слова и количество топиков, возврашает темы\n",
    "    '''\n",
    "    with open('topic_paper_for_1935.txt', 'w', encoding='utf-8') as f:\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            print(f\"Тема {topic_idx+1}:\", file=f)\n",
    "            topic_words = \", \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "            print(topic_words, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=5, ngram_range=(2, 2), stop_words=stoplist)\n",
    "    tfidf = tfidf_vectorizer.fit_transform(paper_corpus_lemmatized[1935])\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    nmf = NMF(n_components=10, random_state=42, alpha=.1, l1_ratio=.5, init='nndsvd', max_iter=100000).fit(tfidf)\n",
    "    get_topics(nmf, tfidf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()#nndsvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ключевые слова по degree centrality, решил не использовать\n",
    "import networkx as nx\n",
    "def get_keywords_graph_degree_centrality(text, num_keywords):\n",
    "    \"\"\"\n",
    "    берет текст в виде списка слов и количество ключевых слов для извлечения,\n",
    "    возвращает в качестве ключевых слов узлы графа с самой высокой степенью degree centrality\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from([(text[i], text[i+1]) for i, item in enumerate(text) if i != (len(text)-1)])\n",
    "    dc = nx.degree_centrality(G)\n",
    "    keywords = [node for node in sorted(dc, key=dc.get, reverse=True)[:num_keywords]]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_bigrams_tf_idf(texts, num_keywords):\n",
    "    \"\"\"\n",
    "    берет список не лемматизированных текстов в виде строк\n",
    "    и количество ключевых слов для извлечения,\n",
    "    возвращает в качестве ключевых слов слова и биграммы, \n",
    "    отобранные методом TF-IDF\n",
    "    \"\"\"\n",
    "    keywords = []\n",
    "    make_tf_idf = TfidfVectorizer(stop_words=rus_stop_words, ngram_range=(1, 2))\n",
    "    texts_as_tfidf_vectors = make_tf_idf.fit_transform(get_preprocessed_text(text) for text in texts)\n",
    "    id2word = {i : word for i, word in enumerate(make_tf_idf.get_feature_names())} \n",
    "    for text_row in range(texts_as_tfidf_vectors.shape[0]): \n",
    "        row_data = texts_as_tfidf_vectors.getrow(text_row) \n",
    "        words_for_this_text = row_data.toarray().argsort() \n",
    "        top_words_for_this_text = words_for_this_text [0, : -(num_keywords + 1) : -1] \n",
    "        keywords_for_this_text = [id2word[w] for w in top_words_for_this_text]\n",
    "        keywords.append(keywords_for_this_text)\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_keywords in get_keywords_bigrams_tf_idf(texts, 10):\n",
    "    print(text_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_fd = nltk.FreqDist(flat_dict_papers[1930])\n",
    "bigram_fd = nltk.FreqDist(nltk.bigrams(flat_dict_papers[1930]))\n",
    "# tokens = nltk.wordpunct_tokenize(text)\n",
    "finder = BigramCollocationFinder(word_fd, bigram_fd)\n",
    "# sorted(finder.ngram_fd.items(), key=lambda t: (-t[1], t[0]))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(flat_prozhito[1930])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(flat_prozhito[1930])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функций, которая выдаёт самые частотные коллокации по убыванию\n",
    "def collocater(text):\n",
    "    tokens = nltk.wordpunct_tokenize(text)\n",
    "    tokens_without_punkt = [token for token in tokens if token not in extended_punctuation and stoplist]\n",
    "    word_prozhito = nltk.FreqDist(tokens_without_punkt)\n",
    "    bigram_prozhito = nltk.FreqDist(nltk.bigrams(tokens_without_punkt))\n",
    "    finder_prozhito = BigramCollocationFinder(word_prozhito, bigram_prozhito)\n",
    "    finder_prozhito.apply_word_filter(lambda w: len(w) < 3 or w.lower() in stoplist)\n",
    "    finder_prozhito = sorted(finder_prozhito.ngram_fd.items(), key=lambda t: (-t[1], t[0]))\n",
    "    return finder_prozhito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_prozhito_without_punkt[1930]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colloc_for_years_all = []\n",
    "for key, value in flat_prozhito_without_punkt.items():\n",
    "    colloc_for_years.append('\\n' + str(key) + '\\n')\n",
    "    colloc_for_years.append(collocater(flat_prozhito_without_punkt[key]))\n",
    "# with open('colloc_for_years_prozhito50.txt', 'w', encoding='utf-8') as f:\n",
    "#     print(*colloc_for_years, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(finder_prozhito.ngram_fd.items(), key=lambda t: (-t[1], t[0]))[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_prozhito[1930][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('years_diaries_clean_dic.pickle', 'rb') as f:\n",
    "    years_diaries_clean_dic = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(finder.ngram_fd.items(), key=lambda t: (-t[1], t[0]))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1942 - всего 3 записи, не делает моделирование, 1935 тоже - 1\n",
    "len(flat_dict_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, papers in flat_dict_papers.items():\n",
    "    with open(f'Newspaper_for_{year}.txt', 'w', encoding='utf-8') as f:\n",
    "        print(*papers, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colloc_for_years_papers_all = []\n",
    "for key, value in flat_dict_papers.items():\n",
    "    colloc_for_years_papers.append('\\n' + str(key) + '\\n')\n",
    "    colloc_for_years_papers.append(collocater(' '.join(flat_dict_papers[key]), 50))\n",
    "with open('colloc_for_years_papers50.txt', 'w', encoding='utf-8') as f:\n",
    "    print(*colloc_for_years_papers, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(flat_dict_papers[1928][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_nkrya = []\n",
    "date = None\n",
    "author_nkrya = None\n",
    "for file_root, dirs, files in os.walk('.'):\n",
    "    for file in files:\n",
    "        if file.endswith('.xhtml') or file.endswith('.xml'):\n",
    "            tree = ET.parse(os.path.join(file_root, file))\n",
    "            root = tree.getroot()\n",
    "            for tag in root.iter('meta'):\n",
    "                if tag.attrib['name'] in {'date', 'publ_year', 'created'}:\n",
    "                    year = re.search(r'\\d{4}', tag.attrib['content'])\n",
    "                    if year:\n",
    "                        if int(year[0]) in range(1928, 1954):\n",
    "                            date = int(year[0])\n",
    "                elif tag.attrib['name'] == 'author':\n",
    "                    author_nkrya = tag.attrib['content']\n",
    "            if date and author_nkrya:\n",
    "                authors_nkrya.append(author_nkrya)\n",
    "            date = None\n",
    "            author_nkrya = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dict(Counter(authors_nkrya)).items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = 'шахтинский'\n",
    "bigram_for_years_prozhito = {}\n",
    "for key, value in flat_prozhito_without_punkt.items():\n",
    "    bigram_for_years_prozhito[key] = flat_prozhito_without_punkt[key].count(bigram)\n",
    "print(bigram_for_years_prozhito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(list(bigram_for_years_prozhito.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_for_years_nkrya = {}\n",
    "for key, value in sorted(flat_dict_papers.items()):\n",
    "    bigram_for_years_nkrya[key] = ' '.join(flat_dict_papers[key]).count(bigram)\n",
    "print(bigram_for_years_nkrya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import style  # добавляем стили\n",
    "style.use('ggplot')  # выбираем стиль ggplot\n",
    "\n",
    "plt.plot(list(bigram_for_years_nkrya.keys()), list(bigram_for_years_nkrya.values()), 'g',label='Публицистика НКРЯ')\n",
    "plt.plot(list(bigram_for_years_prozhito.keys()), list(bigram_for_years_prozhito.values()), 'r',label='\"Прожито\"')\n",
    "\n",
    "plt.title(bigram)\n",
    "plt.ylabel('Количество вхождений униграммы/биграммы')\n",
    "plt.xlabel('Год')\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('испания.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics = []\n",
    "for file_root, dirs, files in os.walk('.'):\n",
    "    for file in files:\n",
    "        if file.endswith('.xhtml') or file.endswith('.xml'):\n",
    "            tree = ET.parse(os.path.join(file_root, file))\n",
    "            root = tree.getroot()\n",
    "            for tag in root.iter('meta'):\n",
    "                if tag.attrib['name'] in {'date', 'publ_year', 'created'}:\n",
    "                    year = re.search(r'\\d{4}', tag.attrib['content'])\n",
    "                    if year:\n",
    "                        if int(year[0]) in range(1928, 1954):\n",
    "                            date = int(year[0])\n",
    "                elif tag.attrib['name'] == 'topic':\n",
    "                    topic = tag.attrib['content']\n",
    "            if date and topic:\n",
    "                all_topics.append(topic)\n",
    "            date = None\n",
    "            topic = None\n",
    "sorted(dict(Counter(all_topics)).items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publ_places = []\n",
    "for file_root, dirs, files in os.walk('.'):\n",
    "    for file in files:\n",
    "        if file.endswith('.xhtml') or file.endswith('.xml'):\n",
    "            tree = ET.parse(os.path.join(file_root, file))\n",
    "            root = tree.getroot()\n",
    "            for tag in root.iter('meta'):\n",
    "                if tag.attrib['name'] in {'date', 'publ_year', 'created'}:\n",
    "                    year = re.search(r'\\d{4}', tag.attrib['content'])\n",
    "                    if year:\n",
    "                        if int(year[0]) in range(1928, 1954):\n",
    "                            date = int(year[0])\n",
    "                elif tag.attrib['name'] == 'publication':\n",
    "                    publ_place = tag.attrib['content']\n",
    "            if date and publ_place:\n",
    "                publ_places.append(publ_place)\n",
    "            date = None\n",
    "            publ_place = None\n",
    "sorted(dict(Counter(publ_places)).items(), key=lambda x: x[1], reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
